{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8a4d42-ffe4-4aad-9afa-3da75b63d391",
   "metadata": {},
   "source": [
    "# Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c051aa5-0305-4c08-bcab-7f8a40b46e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install piscis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743137e7-1a2e-4e64-b739-70bd40c30b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import nd2\n",
    "import numpy as np\n",
    "import requests\n",
    "import xarray as xr\n",
    "\n",
    "from huggingface_hub import HfFileSystem\n",
    "from jax import random\n",
    "from pathlib import Path\n",
    "\n",
    "from piscis.data import generate_dataset\n",
    "from piscis.paths import HF_DATASETS_DIR\n",
    "from piscis.utils import fit_coords, pad_and_stack, remove_duplicate_coords, snap_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865f4d2-0e00-4df1-b24a-3f7d09fae7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to outputs.\n",
    "outputs_path = Path().absolute().parent / 'outputs'\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to datasets.\n",
    "datasets_path = outputs_path / 'datasets'\n",
    "datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to Piscis datasets.\n",
    "piscis_datasets_path = datasets_path / 'piscis'\n",
    "piscis_datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to deepBlink datasets.\n",
    "deepblink_datasets_path = datasets_path / 'deepblink'\n",
    "deepblink_datasets_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553a356-2a0c-41d1-8594-980c4806fd6c",
   "metadata": {},
   "source": [
    "### Generate datasets from NimbusImage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a5d0b-0202-4701-8d26-c0659cde0a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw images and annotations from Hugging Face.\n",
    "dataset_path = piscis_datasets_path / '20230905'\n",
    "hs = HfFileSystem()\n",
    "hs.download(f'{HF_DATASETS_DIR}20230905/raw', str(dataset_path / 'raw'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabccead-ef3f-486a-9b61-dead74cc8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to raw images and annotations.\n",
    "images_path = dataset_path / 'raw' / 'images'\n",
    "annotations_path = dataset_path / 'raw' / 'annotations'\n",
    "\n",
    "# Define path to save the combined dataset.\n",
    "combined_path = dataset_path / 'combined'\n",
    "combined_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to save datasets with DAPI.\n",
    "with_dapi_path = dataset_path / 'with_dapi'\n",
    "with_dapi_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49751cf8-dc38-469e-b933-bf87476dcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all datasets.\n",
    "# datasets_list = [file.stem for file in images_path.glob('*.nd2')]\n",
    "\n",
    "# # Loop over datasets.\n",
    "# for dataset in datasets_list:\n",
    "\n",
    "#     # Load ND2 image.\n",
    "#     image = nd2.imread(images_path / f'{dataset}.nd2', dask=True, xarray=True)\n",
    "#     channel_names = tuple(image.coords['C'].to_numpy())\n",
    "#     dapi_channel_index = next((i for i, name in enumerate(channel_names) if name.startswith('DAPI')), None)\n",
    "#     image.coords['channel_names'] = image.coords['C']\n",
    "#     for axis, size in image.sizes.items():\n",
    "#         image.coords[axis] = np.arange(size)\n",
    "#     image.attrs['axes_calibration'] = image.attrs.pop('metadata')['metadata'].channels[0].volume.axesCalibration\n",
    "\n",
    "#     # Load annotations.\n",
    "#     with open(annotations_path / f'{dataset}.json') as f:\n",
    "#         annotations = json.load(f)['annotations']\n",
    "\n",
    "#     # Group point annotations by channel and frame.\n",
    "#     frames_dict = {'C': set(), 'P': set(), 'Z': set(), 'T': set()}\n",
    "#     if dapi_channel_index is not None:\n",
    "#         frames_dict['C'].add(dapi_channel_index)\n",
    "#     for annotation in annotations:\n",
    "#         if annotation['shape'] == 'point':\n",
    "#             location = annotation['location']\n",
    "#             channel_index = annotation['channel']\n",
    "#             frames_dict['C'].add(channel_index)\n",
    "#             frames_dict['P'].add(location['XY'])\n",
    "#             frames_dict['Z'].add(location['Z'])\n",
    "#             frames_dict['T'].add(location['Time'])\n",
    "\n",
    "#     # Save subsetted image.\n",
    "#     image = image.isel(**{k: list(frames_dict[k]) for k in image.coords if k not in ['channel_names', 'Y', 'X']})\n",
    "#     image.to_netcdf(images_path / f'{dataset}.nc', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28a5ce-3ab2-42fb-9bbc-92bfdee26400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axes order.\n",
    "axes_order = ('P', 'Z', 'T', 'C')\n",
    "\n",
    "# List all datasets.\n",
    "datasets_list = [file.stem for file in images_path.glob('*.nc')]\n",
    "\n",
    "# Loop over datasets.\n",
    "for dataset in datasets_list:\n",
    "\n",
    "    # Convert the dataset name to a MD5 hash.\n",
    "    hashed = hashlib.md5(dataset.encode()).hexdigest()\n",
    "    \n",
    "    # Convert the first 8 characters of the hash to an integer to create a seed.\n",
    "    seed = int(hashed[:8], 16)\n",
    "    \n",
    "    # Generate a random key.\n",
    "    key = random.PRNGKey(seed)\n",
    "\n",
    "    # Load ND2 metadata and image.\n",
    "    image = xr.load_dataarray(images_path / f'{dataset}.nc')\n",
    "    channel_names = tuple(image.coords['channel_names'].to_numpy())\n",
    "    dapi_channel_index = next((i for i, name in enumerate(channel_names) if name.startswith('DAPI')), None)\n",
    "\n",
    "    # Load annotations.\n",
    "    with open(annotations_path / f'{dataset}.json') as f:\n",
    "        annotations = json.load(f)['annotations']\n",
    "\n",
    "    # Group point annotations by channel and frame.\n",
    "    coords_dict = {}\n",
    "    for annotation in annotations:\n",
    "        if annotation['shape'] == 'point':\n",
    "            location = annotation['location']\n",
    "            channel_index = np.argwhere(image.coords['C'].to_numpy() == annotation['channel'])[0, 0]\n",
    "            coordinates = annotation['coordinates'][0]\n",
    "            frame_index = []\n",
    "            for axis in image.sizes:\n",
    "                if axis == 'P':\n",
    "                    frame_index.append(np.argwhere(image.coords['P'].to_numpy() == location['XY'])[0, 0])\n",
    "                elif axis == 'Z':\n",
    "                    frame_index.append(np.argwhere(image.coords['Z'].to_numpy() == location['Z'])[0, 0])\n",
    "                elif axis == 'T':\n",
    "                    frame_index.append(np.argwhere(image.coords['T'].to_numpy() == location['Time'])[0, 0])\n",
    "                elif axis == 'C':\n",
    "                    frame_index.append(channel_index)\n",
    "            frame_index = tuple(frame_index)\n",
    "            coords_dict.setdefault(channel_index, {})\n",
    "            coords_dict[channel_index].setdefault(frame_index, [])\n",
    "            coords_dict[channel_index][frame_index].append((coordinates['y'] - 0.5, coordinates['x'] - 0.5))\n",
    "\n",
    "    # Loop over annotations by channel.\n",
    "    for channel_index, channel_coords_dict in coords_dict.items():\n",
    "\n",
    "        # Create empty lists.\n",
    "        fish_images_list = []\n",
    "        with_dapi_images_list = []\n",
    "        coords_list = []\n",
    "\n",
    "        # Loop over annotations by frame.\n",
    "        for frame_index, coords in channel_coords_dict.items():\n",
    "\n",
    "            # Obtain FISH image.\n",
    "            fish_image = image[frame_index].compute()\n",
    "\n",
    "            if dapi_channel_index is not None:\n",
    "\n",
    "                # Construct index for DAPI frame.\n",
    "                dapi_index = (*frame_index[:-1], dapi_channel_index)\n",
    "\n",
    "                # Obtain DAPI image.\n",
    "                dapi_image = image[dapi_index].compute()\n",
    "\n",
    "            # Process coordinates by snapping, fitting, and removal of duplicates.\n",
    "            coords = np.array(coords)\n",
    "            coords = snap_coords(coords, fish_image)\n",
    "            coords = fit_coords(coords, fish_image)\n",
    "            coords = remove_duplicate_coords(coords)\n",
    "\n",
    "            # Add images and processed coordinates.\n",
    "            fish_images_list.append(fish_image)\n",
    "            with_dapi_images_list.append(np.stack((fish_image, dapi_image)))\n",
    "            coords_list.append(coords)\n",
    "\n",
    "        # Generate dataset.\n",
    "        generate_dataset(dataset_path / f'{dataset}_{channel_names[channel_index]}.npz', fish_images_list, coords_list, key)\n",
    "\n",
    "        # Generate dataset with DAPI if possible.\n",
    "        if dapi_channel_index is not None:\n",
    "            generate_dataset(with_dapi_path / f'{dataset}_{channel_names[channel_index]}_with_dapi.npz', with_dapi_images_list, coords_list, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fe805-9023-43a8-8715-d3830ab38269",
   "metadata": {},
   "source": [
    "### Subset deepBlink datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bba91-24fd-4218-b69b-3b7d31e0c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the Figshare API.\n",
    "api_url = f'https://api.figshare.com/v2/articles/12958037'\n",
    "\n",
    "# Get a list of files from Figshare.\n",
    "files = requests.get(api_url).json()['files']\n",
    "\n",
    "for file in files:\n",
    "    file_name = file['name']\n",
    "    if file_name.endswith('.npz'):\n",
    "        download_url = file['download_url']\n",
    "        response = requests.get(download_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(deepblink_datasets_path / file_name, 'wb') as handle:\n",
    "            for block in response.iter_content(1024):\n",
    "                handle.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb3952-b03e-4c03-8972-106f55ed92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define define train-valid-test split.\n",
    "splits = {'train': 42, 'valid': 9, 'test': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a96c28-f4b1-4321-9bbb-ece323881cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over deepBlink datasets.\n",
    "for deepblink_dataset in ('particle', 'microtubule', 'receptor', 'vesicle'):\n",
    "\n",
    "    # Load deepBlink dataset.\n",
    "    deepblink_ds = dict(np.load(deepblink_datasets_path / f'{deepblink_dataset}.npz', allow_pickle=True))\n",
    "\n",
    "    # Loop through splits.\n",
    "    for k, v in deepblink_ds.items():\n",
    "\n",
    "        if k.startswith('x'):\n",
    "\n",
    "            # Extract the split type.\n",
    "            split = k.split('_')[1]\n",
    "\n",
    "            # Get coordinates.\n",
    "            v_y = deepblink_ds[f'y_{split}']\n",
    "\n",
    "            # Use only SNR=1 images for microtubule, receptor, and vesicle datasets.\n",
    "            len_v = len(v)\n",
    "            if deepblink_dataset in ('microtubule', 'receptor', 'vesicle'):\n",
    "                len_v = len_v // 4\n",
    "                v = v[:len_v]\n",
    "                v_y = v_y[:len_v]\n",
    "\n",
    "            # Extract the desired size of this split.\n",
    "            n = splits[split]\n",
    "\n",
    "            # Determine the necessary stride.\n",
    "            stride = (len_v - 1) // (n - 1)\n",
    "\n",
    "            # Subset images.\n",
    "            images_subset = np.empty(n, dtype=object)\n",
    "            images_subset[:] = list(v[::stride][:n][:, 128:384, 128:384])\n",
    "            deepblink_ds[k] = images_subset\n",
    "\n",
    "            # Subset coordinates.\n",
    "            coords_subset = v_y[::stride][:n]\n",
    "            for i, coords in enumerate(coords_subset):\n",
    "                coords = coords - np.array((128, 128))\n",
    "                coords_subset[i] = coords[np.all(coords >= -0.5, axis=-1) & np.all(coords <= 255.5, axis=-1)]\n",
    "            deepblink_ds[f'y_{split}'] = coords_subset\n",
    "\n",
    "    # Save subset of the deepBlink dataset.\n",
    "    np.savez(dataset_path / f'{deepblink_dataset}_subset.npz', **deepblink_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8335d-ed62-4716-abd8-1d355c0353f4",
   "metadata": {},
   "source": [
    "### Generate a combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a1e51-c0fa-4bf0-ae83-838813e4883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# List all npz files.\n",
    "npz_list = [file for file in dataset_path.glob('*.npz')]\n",
    "\n",
    "# Loop over each npz file.\n",
    "for npz in npz_list:\n",
    "\n",
    "    # Load dataset.\n",
    "    ds = np.load(npz, allow_pickle=True)\n",
    "\n",
    "    # Add images and coords for each split.\n",
    "    x_train.append(ds['x_train'])\n",
    "    y_train.append(ds['y_train'])\n",
    "    x_valid.append(ds['x_valid'])\n",
    "    y_valid.append(ds['y_valid'])\n",
    "    x_test.append(ds['x_test'])\n",
    "    y_test.append(ds['y_test'])\n",
    "\n",
    "# Concatenate across datasets.\n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "x_valid = np.concatenate(x_valid)\n",
    "y_valid = np.concatenate(y_valid)\n",
    "x_test = np.concatenate(x_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "# Pad and stack images.\n",
    "x_train = pad_and_stack(x_train)\n",
    "x_valid = pad_and_stack(x_valid)\n",
    "x_test = pad_and_stack(x_test)\n",
    "\n",
    "# Generate a combined dataset.\n",
    "np.savez(combined_path / f'{dataset_path.stem}_combined.npz', x_train=x_train, y_train=y_train, x_valid=x_valid, y_valid=y_valid, x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfd75f-9df4-4e58-a5c7-61c6f9601fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
